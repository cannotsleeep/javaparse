[{"input":"","instruction":"<line1>{\n<line2>    httpServer = MockHttpServer.createHttp(new InetSocketAddress(InetAddress.getLoopbackAddress(), 0), 0);\n<line3>    ThreadFactory threadFactory = EsExecutors.daemonThreadFactory(\"[\" + ESMockAPIBasedRepositoryIntegTestCase.class.getName() + \"]\");\n<line4>    // the EncryptedRepository can require more than one connection open at one time\n<line5>    executorService = EsExecutors.newScaling(ESMockAPIBasedRepositoryIntegTestCase.class.getName(), 0, 2, 60, TimeUnit.SECONDS, true, threadFactory, new ThreadContext(Settings.EMPTY));\n<line6>    httpServer.setExecutor(r -> {\n<line7>        executorService.execute(() -> {\n<line8>            try {\n<line9>                r.run();\n<line10>            } catch (Throwable t) {\n<line11>                throw t;\n<line12>            }\n<line13>        });\n<line14>    });\n<line15>    httpServer.start();\n<line16>}","output":"<line1>{\n<line2>    httpServer = MockHttpServer.createHttp(new InetSocketAddress(InetAddress.getLoopbackAddress(), 0), 0);\n<line3>    ThreadFactory threadFactory = EsExecutors.daemonThreadFactory(\"[\" + ESMockAPIBasedRepositoryIntegTestCase.class.getName() + \"]\");\n<line4>    // the EncryptedRepository can require more than one connection open at one time\n<line5>    executorService = EsExecutors.newScaling(ESMockAPIBasedRepositoryIntegTestCase.class.getName(), 0, 2, 60, TimeUnit.SECONDS, true, threadFactory, new ThreadContext(Settings.EMPTY));\n<line6>    httpServer.setExecutor(r -> {\n<line7>        executorService.execute(() -> {\n<line8>            try {\n<line9>                r.run();\n<line10>            } catch (Throwable t) {\n<line11>                log.error(\"Error in execution on mock http server IO thread\", t);\n<line12>                throw t;\n<line13>            }\n<line14>        });\n<line15>    });\n<line16>    httpServer.start();\n<line17>}"},{"input":"","instruction":"<line1>{\n<line2>    executorService.execute(() -> {\n<line3>        try {\n<line4>            r.run();\n<line5>        } catch (Throwable t) {\n<line6>            throw t;\n<line7>        }\n<line8>    });\n<line9>}","output":"<line1>{\n<line2>    executorService.execute(() -> {\n<line3>        try {\n<line4>            r.run();\n<line5>        } catch (Throwable t) {\n<line6>            log.error(\"Error in execution on mock http server IO thread\", t);\n<line7>            throw t;\n<line8>        }\n<line9>    });\n<line10>}"},{"input":"","instruction":"<line1>{\n<line2>    try {\n<line3>        r.run();\n<line4>    } catch (Throwable t) {\n<line5>        throw t;\n<line6>    }\n<line7>}","output":"<line1>{\n<line2>    try {\n<line3>        r.run();\n<line4>    } catch (Throwable t) {\n<line5>        log.error(\"Error in execution on mock http server IO thread\", t);\n<line6>        throw t;\n<line7>    }\n<line8>}"},{"input":"","instruction":"<line1>{\n<line2>    throw t;\n<line3>}","output":"<line1>{\n<line2>    log.error(\"Error in execution on mock http server IO thread\", t);\n<line3>    throw t;\n<line4>}"},{"input":"","instruction":"<line1>{\n<line2>    if ((TYPE.equals(type) == false) || (operation.origin() != Origin.PRIMARY)) {\n<line3>        return operation;\n<line4>    }\n<line5>    try {\n<line6>        if (ALLOWED_OPERATIONS.tryAcquire(30, TimeUnit.SECONDS)) {\n<line7>            return operation;\n<line8>        }\n<line9>    } catch (InterruptedException e) {\n<line10>        throw new RuntimeException(e);\n<line11>    }\n<line12>    throw new IllegalStateException(\"Something went wrong\");\n<line13>}","output":"<line1>{\n<line2>    if ((TYPE.equals(type) == false) || (operation.origin() != Origin.PRIMARY)) {\n<line3>        return operation;\n<line4>    }\n<line5>    try {\n<line6>        log.debug(\"checking\");\n<line7>        if (ALLOWED_OPERATIONS.tryAcquire(30, TimeUnit.SECONDS)) {\n<line8>            log.debug(\"passed\");\n<line9>            return operation;\n<line10>        }\n<line11>    } catch (InterruptedException e) {\n<line12>        throw new RuntimeException(e);\n<line13>    }\n<line14>    throw new IllegalStateException(\"Something went wrong\");\n<line15>}"},{"input":"","instruction":"<line1>{\n<line2>    if (ALLOWED_OPERATIONS.tryAcquire(30, TimeUnit.SECONDS)) {\n<line3>        return operation;\n<line4>    }\n<line5>}","output":"<line1>{\n<line2>    log.debug(\"checking\");\n<line3>    if (ALLOWED_OPERATIONS.tryAcquire(30, TimeUnit.SECONDS)) {\n<line4>        log.debug(\"passed\");\n<line5>        return operation;\n<line6>    }\n<line7>}"},{"input":"","instruction":"<line1>{\n<line2>    return operation;\n<line3>}","output":"<line1>{\n<line2>    log.debug(\"passed\");\n<line3>    return operation;\n<line4>}"},{"input":"","instruction":"<line1>{\n<line2>    }\n<line3>    return invokeParser(eql, params, EqlBaseParser::singleStatement, AstBuilder::plan);\n<line4>}","output":"<line1>{\n<line2>    if (log.isDebugEnabled()) {\n<line3>        log.debug(\"Parsing as statement: {}\", eql);\n<line4>    }\n<line5>    return invokeParser(eql, params, EqlBaseParser::singleStatement, AstBuilder::plan);\n<line6>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"Parsing as statement: {}\", eql);\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    }\n<line3>    return invokeParser(expression, params, EqlBaseParser::singleExpression, AstBuilder::expression);\n<line4>}","output":"<line1>{\n<line2>    if (log.isDebugEnabled()) {\n<line3>        log.debug(\"Parsing as expression: {}\", expression);\n<line4>    }\n<line5>    return invokeParser(expression, params, EqlBaseParser::singleExpression, AstBuilder::expression);\n<line6>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"Parsing as expression: {}\", expression);\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    try {\n<line3>        EqlBaseLexer lexer = new EqlBaseLexer(new ANTLRInputStream(eql));\n<line4>        lexer.removeErrorListeners();\n<line5>        lexer.addErrorListener(ERROR_LISTENER);\n<line6>        CommonTokenStream tokenStream = new CommonTokenStream(lexer);\n<line7>        EqlBaseParser parser = new EqlBaseParser(tokenStream);\n<line8>        parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line9>        parser.removeErrorListeners();\n<line10>        parser.addErrorListener(ERROR_LISTENER);\n<line11>        parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line12>        if (DEBUG) {\n<line13>            debug(parser);\n<line14>            tokenStream.fill();\n<line15>            for (Token t : tokenStream.getTokens()) {\n<line16>                String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line17>                String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line18>            }\n<line19>        }\n<line20>        ParserRuleContext tree = parseFunction.apply(parser);\n<line21>        if (DEBUG) {\n<line22>        }\n<line23>        return visitor.apply(new AstBuilder(params), tree);\n<line24>    } catch (StackOverflowError e) {\n<line25>        throw new ParsingException(\"EQL statement is too large, \" + \"causing stack overflow when generating the parsing tree: [{}]\", eql);\n<line26>    }\n<line27>}","output":"<line1>{\n<line2>    try {\n<line3>        EqlBaseLexer lexer = new EqlBaseLexer(new ANTLRInputStream(eql));\n<line4>        lexer.removeErrorListeners();\n<line5>        lexer.addErrorListener(ERROR_LISTENER);\n<line6>        CommonTokenStream tokenStream = new CommonTokenStream(lexer);\n<line7>        EqlBaseParser parser = new EqlBaseParser(tokenStream);\n<line8>        parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line9>        parser.removeErrorListeners();\n<line10>        parser.addErrorListener(ERROR_LISTENER);\n<line11>        parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line12>        if (DEBUG) {\n<line13>            debug(parser);\n<line14>            tokenStream.fill();\n<line15>            for (Token t : tokenStream.getTokens()) {\n<line16>                String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line17>                String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line18>                log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line19>            }\n<line20>        }\n<line21>        ParserRuleContext tree = parseFunction.apply(parser);\n<line22>        if (DEBUG) {\n<line23>            log.info(\"Parse tree {} \" + tree.toStringTree());\n<line24>        }\n<line25>        return visitor.apply(new AstBuilder(params), tree);\n<line26>    } catch (StackOverflowError e) {\n<line27>        throw new ParsingException(\"EQL statement is too large, \" + \"causing stack overflow when generating the parsing tree: [{}]\", eql);\n<line28>    }\n<line29>}"},{"input":"","instruction":"<line1>{\n<line2>    EqlBaseLexer lexer = new EqlBaseLexer(new ANTLRInputStream(eql));\n<line3>    lexer.removeErrorListeners();\n<line4>    lexer.addErrorListener(ERROR_LISTENER);\n<line5>    CommonTokenStream tokenStream = new CommonTokenStream(lexer);\n<line6>    EqlBaseParser parser = new EqlBaseParser(tokenStream);\n<line7>    parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line8>    parser.removeErrorListeners();\n<line9>    parser.addErrorListener(ERROR_LISTENER);\n<line10>    parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line11>    if (DEBUG) {\n<line12>        debug(parser);\n<line13>        tokenStream.fill();\n<line14>        for (Token t : tokenStream.getTokens()) {\n<line15>            String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line16>            String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line17>        }\n<line18>    }\n<line19>    ParserRuleContext tree = parseFunction.apply(parser);\n<line20>    if (DEBUG) {\n<line21>    }\n<line22>    return visitor.apply(new AstBuilder(params), tree);\n<line23>}","output":"<line1>{\n<line2>    EqlBaseLexer lexer = new EqlBaseLexer(new ANTLRInputStream(eql));\n<line3>    lexer.removeErrorListeners();\n<line4>    lexer.addErrorListener(ERROR_LISTENER);\n<line5>    CommonTokenStream tokenStream = new CommonTokenStream(lexer);\n<line6>    EqlBaseParser parser = new EqlBaseParser(tokenStream);\n<line7>    parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line8>    parser.removeErrorListeners();\n<line9>    parser.addErrorListener(ERROR_LISTENER);\n<line10>    parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line11>    if (DEBUG) {\n<line12>        debug(parser);\n<line13>        tokenStream.fill();\n<line14>        for (Token t : tokenStream.getTokens()) {\n<line15>            String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line16>            String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line17>            log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line18>        }\n<line19>    }\n<line20>    ParserRuleContext tree = parseFunction.apply(parser);\n<line21>    if (DEBUG) {\n<line22>        log.info(\"Parse tree {} \" + tree.toStringTree());\n<line23>    }\n<line24>    return visitor.apply(new AstBuilder(params), tree);\n<line25>}"},{"input":"","instruction":"<line1>{\n<line2>    debug(parser);\n<line3>    tokenStream.fill();\n<line4>    for (Token t : tokenStream.getTokens()) {\n<line5>        String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line6>        String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line7>    }\n<line8>}","output":"<line1>{\n<line2>    debug(parser);\n<line3>    tokenStream.fill();\n<line4>    for (Token t : tokenStream.getTokens()) {\n<line5>        String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line6>        String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line7>        log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line8>    }\n<line9>}"},{"input":"","instruction":"<line1>{\n<line2>    String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line3>    String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line4>}","output":"<line1>{\n<line2>    String symbolicName = EqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line3>    String literalName = EqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line4>    log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line5>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.info(\"Parse tree {} \" + tree.toStringTree());\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    Holder<Boolean> retrySecondTime = new Holder<Boolean>(false);\n<line3>    queryRunner.accept(e -> {\n<line4>        // the search request likely ran on nodes with different versions of ES\n<line5>        // we will retry on a node with an older version that should generate a backwards compatible _search request\n<line6>        if (e instanceof SearchPhaseExecutionException && ((SearchPhaseExecutionException) e).getCause() instanceof VersionMismatchException) {\n<line7>            }\n<line8>            DiscoveryNode localNode = clusterService.state().nodes().getLocalNode();\n<line9>            DiscoveryNode candidateNode = null;\n<line10>            for (DiscoveryNode node : clusterService.state().nodes()) {\n<line11>                // find the first node that's older than the current node\n<line12>                if (node != localNode && node.getVersion().before(localNode.getVersion())) {\n<line13>                    candidateNode = node;\n<line14>                    break;\n<line15>                }\n<line16>            }\n<line17>            if (candidateNode != null) {\n<line18>                }\n<line19>                // re-send the request to the older node\n<line20>                retryRequest.accept(candidateNode);\n<line21>            } else {\n<line22>                retrySecondTime.set(true);\n<line23>            }\n<line24>        } else {\n<line25>            onFailure.accept(e);\n<line26>        }\n<line27>    });\n<line28>    if (retrySecondTime.get()) {\n<line29>        }\n<line30>        queryRunner.accept(onFailure);\n<line31>    }\n<line32>}","output":"<line1>{\n<line2>    Holder<Boolean> retrySecondTime = new Holder<Boolean>(false);\n<line3>    queryRunner.accept(e -> {\n<line4>        // the search request likely ran on nodes with different versions of ES\n<line5>        // we will retry on a node with an older version that should generate a backwards compatible _search request\n<line6>        if (e instanceof SearchPhaseExecutionException && ((SearchPhaseExecutionException) e).getCause() instanceof VersionMismatchException) {\n<line7>            if (log.isDebugEnabled()) {\n<line8>                log.debug(\"Caught exception type [{}] with cause [{}].\", e.getClass().getName(), e.getCause());\n<line9>            }\n<line10>            DiscoveryNode localNode = clusterService.state().nodes().getLocalNode();\n<line11>            DiscoveryNode candidateNode = null;\n<line12>            for (DiscoveryNode node : clusterService.state().nodes()) {\n<line13>                // find the first node that's older than the current node\n<line14>                if (node != localNode && node.getVersion().before(localNode.getVersion())) {\n<line15>                    candidateNode = node;\n<line16>                    break;\n<line17>                }\n<line18>            }\n<line19>            if (candidateNode != null) {\n<line20>                if (log.isDebugEnabled()) {\n<line21>                    log.debug(\"Candidate node to resend the request to: address [{}], id [{}], name [{}], version [{}]\", candidateNode.getAddress(), candidateNode.getId(), candidateNode.getName(), candidateNode.getVersion());\n<line22>                }\n<line23>                // re-send the request to the older node\n<line24>                retryRequest.accept(candidateNode);\n<line25>            } else {\n<line26>                retrySecondTime.set(true);\n<line27>            }\n<line28>        } else {\n<line29>            onFailure.accept(e);\n<line30>        }\n<line31>    });\n<line32>    if (retrySecondTime.get()) {\n<line33>        if (log.isDebugEnabled()) {\n<line34>            log.debug(\"No candidate node found, likely all were upgraded in the meantime. Re-trying the original request.\");\n<line35>        }\n<line36>        queryRunner.accept(onFailure);\n<line37>    }\n<line38>}"},{"input":"","instruction":"<line1>{\n<line2>    // the search request likely ran on nodes with different versions of ES\n<line3>    // we will retry on a node with an older version that should generate a backwards compatible _search request\n<line4>    if (e instanceof SearchPhaseExecutionException && ((SearchPhaseExecutionException) e).getCause() instanceof VersionMismatchException) {\n<line5>        }\n<line6>        DiscoveryNode localNode = clusterService.state().nodes().getLocalNode();\n<line7>        DiscoveryNode candidateNode = null;\n<line8>        for (DiscoveryNode node : clusterService.state().nodes()) {\n<line9>            // find the first node that's older than the current node\n<line10>            if (node != localNode && node.getVersion().before(localNode.getVersion())) {\n<line11>                candidateNode = node;\n<line12>                break;\n<line13>            }\n<line14>        }\n<line15>        if (candidateNode != null) {\n<line16>            }\n<line17>            // re-send the request to the older node\n<line18>            retryRequest.accept(candidateNode);\n<line19>        } else {\n<line20>            retrySecondTime.set(true);\n<line21>        }\n<line22>    } else {\n<line23>        onFailure.accept(e);\n<line24>    }\n<line25>}","output":"<line1>{\n<line2>    // the search request likely ran on nodes with different versions of ES\n<line3>    // we will retry on a node with an older version that should generate a backwards compatible _search request\n<line4>    if (e instanceof SearchPhaseExecutionException && ((SearchPhaseExecutionException) e).getCause() instanceof VersionMismatchException) {\n<line5>        if (log.isDebugEnabled()) {\n<line6>            log.debug(\"Caught exception type [{}] with cause [{}].\", e.getClass().getName(), e.getCause());\n<line7>        }\n<line8>        DiscoveryNode localNode = clusterService.state().nodes().getLocalNode();\n<line9>        DiscoveryNode candidateNode = null;\n<line10>        for (DiscoveryNode node : clusterService.state().nodes()) {\n<line11>            // find the first node that's older than the current node\n<line12>            if (node != localNode && node.getVersion().before(localNode.getVersion())) {\n<line13>                candidateNode = node;\n<line14>                break;\n<line15>            }\n<line16>        }\n<line17>        if (candidateNode != null) {\n<line18>            if (log.isDebugEnabled()) {\n<line19>                log.debug(\"Candidate node to resend the request to: address [{}], id [{}], name [{}], version [{}]\", candidateNode.getAddress(), candidateNode.getId(), candidateNode.getName(), candidateNode.getVersion());\n<line20>            }\n<line21>            // re-send the request to the older node\n<line22>            retryRequest.accept(candidateNode);\n<line23>        } else {\n<line24>            retrySecondTime.set(true);\n<line25>        }\n<line26>    } else {\n<line27>        onFailure.accept(e);\n<line28>    }\n<line29>}"},{"input":"","instruction":"<line1>{\n<line2>    }\n<line3>    DiscoveryNode localNode = clusterService.state().nodes().getLocalNode();\n<line4>    DiscoveryNode candidateNode = null;\n<line5>    for (DiscoveryNode node : clusterService.state().nodes()) {\n<line6>        // find the first node that's older than the current node\n<line7>        if (node != localNode && node.getVersion().before(localNode.getVersion())) {\n<line8>            candidateNode = node;\n<line9>            break;\n<line10>        }\n<line11>    }\n<line12>    if (candidateNode != null) {\n<line13>        }\n<line14>        // re-send the request to the older node\n<line15>        retryRequest.accept(candidateNode);\n<line16>    } else {\n<line17>        retrySecondTime.set(true);\n<line18>    }\n<line19>}","output":"<line1>{\n<line2>    if (log.isDebugEnabled()) {\n<line3>        log.debug(\"Caught exception type [{}] with cause [{}].\", e.getClass().getName(), e.getCause());\n<line4>    }\n<line5>    DiscoveryNode localNode = clusterService.state().nodes().getLocalNode();\n<line6>    DiscoveryNode candidateNode = null;\n<line7>    for (DiscoveryNode node : clusterService.state().nodes()) {\n<line8>        // find the first node that's older than the current node\n<line9>        if (node != localNode && node.getVersion().before(localNode.getVersion())) {\n<line10>            candidateNode = node;\n<line11>            break;\n<line12>        }\n<line13>    }\n<line14>    if (candidateNode != null) {\n<line15>        if (log.isDebugEnabled()) {\n<line16>            log.debug(\"Candidate node to resend the request to: address [{}], id [{}], name [{}], version [{}]\", candidateNode.getAddress(), candidateNode.getId(), candidateNode.getName(), candidateNode.getVersion());\n<line17>        }\n<line18>        // re-send the request to the older node\n<line19>        retryRequest.accept(candidateNode);\n<line20>    } else {\n<line21>        retrySecondTime.set(true);\n<line22>    }\n<line23>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"Caught exception type [{}] with cause [{}].\", e.getClass().getName(), e.getCause());\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    }\n<line3>    // re-send the request to the older node\n<line4>    retryRequest.accept(candidateNode);\n<line5>}","output":"<line1>{\n<line2>    if (log.isDebugEnabled()) {\n<line3>        log.debug(\"Candidate node to resend the request to: address [{}], id [{}], name [{}], version [{}]\", candidateNode.getAddress(), candidateNode.getId(), candidateNode.getName(), candidateNode.getVersion());\n<line4>    }\n<line5>    // re-send the request to the older node\n<line6>    retryRequest.accept(candidateNode);\n<line7>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"Candidate node to resend the request to: address [{}], id [{}], name [{}], version [{}]\", candidateNode.getAddress(), candidateNode.getId(), candidateNode.getName(), candidateNode.getVersion());\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    }\n<line3>    queryRunner.accept(onFailure);\n<line4>}","output":"<line1>{\n<line2>    if (log.isDebugEnabled()) {\n<line3>        log.debug(\"No candidate node found, likely all were upgraded in the meantime. Re-trying the original request.\");\n<line4>    }\n<line5>    queryRunner.accept(onFailure);\n<line6>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"No candidate node found, likely all were upgraded in the meantime. Re-trying the original request.\");\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    TreeType currentPlan = plan;\n<line3>    long totalDuration = 0;\n<line4>    Map<Batch, List<Transformation>> transformations = new LinkedHashMap<>();\n<line5>    for (Batch batch : batches) {\n<line6>        int batchRuns = 0;\n<line7>        List<Transformation> tfs = new ArrayList<>();\n<line8>        transformations.put(batch, tfs);\n<line9>        boolean hasChanged = false;\n<line10>        long batchStart = System.currentTimeMillis();\n<line11>        long batchDuration = 0;\n<line12>        // run each batch until no change occurs or the limit is reached\n<line13>        do {\n<line14>            hasChanged = false;\n<line15>            batchRuns++;\n<line16>            for (Rule<?, TreeType> rule : batch.rules) {\n<line17>                }\n<line18>                Transformation tf = new Transformation(currentPlan, rule);\n<line19>                tfs.add(tf);\n<line20>                currentPlan = tf.after;\n<line21>                if (tf.hasChanged()) {\n<line22>                    hasChanged = true;\n<line23>                    }\n<line24>                } else {\n<line25>                    }\n<line26>                }\n<line27>            }\n<line28>            batchDuration = System.currentTimeMillis() - batchStart;\n<line29>        } while (hasChanged && batch.limit.reached(batchRuns) == false);\n<line30>        totalDuration += batchDuration;\n<line31>            TreeType before = plan;\n<line32>            TreeType after = plan;\n<line33>            if (tfs.isEmpty() == false) {\n<line34>                before = tfs.get(0).before;\n<line35>                after = tfs.get(tfs.size() - 1).after;\n<line36>            }\n<line37>        }\n<line38>    }\n<line39>    }\n<line40>    return new ExecutionInfo(plan, currentPlan, transformations);\n<line41>}","output":"<line1>{\n<line2>    TreeType currentPlan = plan;\n<line3>    long totalDuration = 0;\n<line4>    Map<Batch, List<Transformation>> transformations = new LinkedHashMap<>();\n<line5>    for (Batch batch : batches) {\n<line6>        int batchRuns = 0;\n<line7>        List<Transformation> tfs = new ArrayList<>();\n<line8>        transformations.put(batch, tfs);\n<line9>        boolean hasChanged = false;\n<line10>        long batchStart = System.currentTimeMillis();\n<line11>        long batchDuration = 0;\n<line12>        // run each batch until no change occurs or the limit is reached\n<line13>        do {\n<line14>            hasChanged = false;\n<line15>            batchRuns++;\n<line16>            for (Rule<?, TreeType> rule : batch.rules) {\n<line17>                if (log.isTraceEnabled()) {\n<line18>                    log.trace(\"About to apply rule {}\", rule);\n<line19>                }\n<line20>                Transformation tf = new Transformation(currentPlan, rule);\n<line21>                tfs.add(tf);\n<line22>                currentPlan = tf.after;\n<line23>                if (tf.hasChanged()) {\n<line24>                    hasChanged = true;\n<line25>                    if (log.isTraceEnabled()) {\n<line26>                        log.trace(\"Rule {} applied\\n{}\", rule, NodeUtils.diffString(tf.before, tf.after));\n<line27>                    }\n<line28>                } else {\n<line29>                    if (log.isTraceEnabled()) {\n<line30>                        log.trace(\"Rule {} applied w/o changes\", rule);\n<line31>                    }\n<line32>                }\n<line33>            }\n<line34>            batchDuration = System.currentTimeMillis() - batchStart;\n<line35>        } while (hasChanged && batch.limit.reached(batchRuns) == false);\n<line36>        totalDuration += batchDuration;\n<line37>        if (log.isTraceEnabled()) {\n<line38>            TreeType before = plan;\n<line39>            TreeType after = plan;\n<line40>            if (tfs.isEmpty() == false) {\n<line41>                before = tfs.get(0).before;\n<line42>                after = tfs.get(tfs.size() - 1).after;\n<line43>            }\n<line44>            log.trace(\"Batch {} applied took {}\\n{}\", batch.name, TimeValue.timeValueMillis(batchDuration), NodeUtils.diffString(before, after));\n<line45>        }\n<line46>    }\n<line47>    if (false == currentPlan.equals(plan) && log.isDebugEnabled()) {\n<line48>        log.debug(\"Tree transformation took {}\\n{}\", TimeValue.timeValueMillis(totalDuration), NodeUtils.diffString(plan, currentPlan));\n<line49>    }\n<line50>    return new ExecutionInfo(plan, currentPlan, transformations);\n<line51>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"Tree transformation took {}\\n{}\", TimeValue.timeValueMillis(totalDuration), NodeUtils.diffString(plan, currentPlan));\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    ResultSetMetaData metadata = rs.getMetaData();\n<line3>    int columns = metadata.getColumnCount();\n<line4>    while (rs.next()) {\n<line5>    }\n<line6>}","output":"<line1>{\n<line2>    ResultSetMetaData metadata = rs.getMetaData();\n<line3>    int columns = metadata.getColumnCount();\n<line4>    while (rs.next()) {\n<line5>        log.info(rowAsString(rs, columns));\n<line6>    }\n<line7>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.info(rowAsString(rs, columns));\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    }\n<line3>    return invokeParser(sql, params, zoneId, SqlBaseParser::singleStatement, AstBuilder::plan);\n<line4>}","output":"<line1>{\n<line2>    if (log.isDebugEnabled()) {\n<line3>        log.debug(\"Parsing as statement: {}\", sql);\n<line4>    }\n<line5>    return invokeParser(sql, params, zoneId, SqlBaseParser::singleStatement, AstBuilder::plan);\n<line6>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"Parsing as statement: {}\", sql);\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    }\n<line3>    return invokeParser(expression, params, UTC, SqlBaseParser::singleExpression, AstBuilder::expression);\n<line4>}","output":"<line1>{\n<line2>    if (log.isDebugEnabled()) {\n<line3>        log.debug(\"Parsing as expression: {}\", expression);\n<line4>    }\n<line5>    return invokeParser(expression, params, UTC, SqlBaseParser::singleExpression, AstBuilder::expression);\n<line6>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.debug(\"Parsing as expression: {}\", expression);\n<line3>}"},{"input":"","instruction":"<line1>{\n<line2>    try {\n<line3>        SqlBaseLexer lexer = new SqlBaseLexer(new CaseChangingCharStream(CharStreams.fromString(sql), true));\n<line4>        lexer.removeErrorListeners();\n<line5>        lexer.addErrorListener(ERROR_LISTENER);\n<line6>        Map<Token, SqlTypedParamValue> paramTokens = new HashMap<>();\n<line7>        TokenSource tokenSource = new ParametrizedTokenSource(lexer, paramTokens, params);\n<line8>        CommonTokenStream tokenStream = new CommonTokenStream(tokenSource);\n<line9>        SqlBaseParser parser = new SqlBaseParser(tokenStream);\n<line10>        parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line11>        parser.removeErrorListeners();\n<line12>        parser.addErrorListener(ERROR_LISTENER);\n<line13>        parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line14>        if (DEBUG) {\n<line15>            debug(parser);\n<line16>            tokenStream.fill();\n<line17>            for (Token t : tokenStream.getTokens()) {\n<line18>                String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line19>                String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line20>            }\n<line21>        }\n<line22>        ParserRuleContext tree = parseFunction.apply(parser);\n<line23>        if (DEBUG) {\n<line24>        }\n<line25>        return visitor.apply(new AstBuilder(paramTokens, zoneId), tree);\n<line26>    } catch (StackOverflowError e) {\n<line27>        throw new ParsingException(\"SQL statement is too large, \" + \"causing stack overflow when generating the parsing tree: [{}]\", sql);\n<line28>    }\n<line29>}","output":"<line1>{\n<line2>    try {\n<line3>        SqlBaseLexer lexer = new SqlBaseLexer(new CaseChangingCharStream(CharStreams.fromString(sql), true));\n<line4>        lexer.removeErrorListeners();\n<line5>        lexer.addErrorListener(ERROR_LISTENER);\n<line6>        Map<Token, SqlTypedParamValue> paramTokens = new HashMap<>();\n<line7>        TokenSource tokenSource = new ParametrizedTokenSource(lexer, paramTokens, params);\n<line8>        CommonTokenStream tokenStream = new CommonTokenStream(tokenSource);\n<line9>        SqlBaseParser parser = new SqlBaseParser(tokenStream);\n<line10>        parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line11>        parser.removeErrorListeners();\n<line12>        parser.addErrorListener(ERROR_LISTENER);\n<line13>        parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line14>        if (DEBUG) {\n<line15>            debug(parser);\n<line16>            tokenStream.fill();\n<line17>            for (Token t : tokenStream.getTokens()) {\n<line18>                String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line19>                String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line20>                log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line21>            }\n<line22>        }\n<line23>        ParserRuleContext tree = parseFunction.apply(parser);\n<line24>        if (DEBUG) {\n<line25>            log.info(\"Parse tree {} \" + tree.toStringTree());\n<line26>        }\n<line27>        return visitor.apply(new AstBuilder(paramTokens, zoneId), tree);\n<line28>    } catch (StackOverflowError e) {\n<line29>        throw new ParsingException(\"SQL statement is too large, \" + \"causing stack overflow when generating the parsing tree: [{}]\", sql);\n<line30>    }\n<line31>}"},{"input":"","instruction":"<line1>{\n<line2>    SqlBaseLexer lexer = new SqlBaseLexer(new CaseChangingCharStream(CharStreams.fromString(sql), true));\n<line3>    lexer.removeErrorListeners();\n<line4>    lexer.addErrorListener(ERROR_LISTENER);\n<line5>    Map<Token, SqlTypedParamValue> paramTokens = new HashMap<>();\n<line6>    TokenSource tokenSource = new ParametrizedTokenSource(lexer, paramTokens, params);\n<line7>    CommonTokenStream tokenStream = new CommonTokenStream(tokenSource);\n<line8>    SqlBaseParser parser = new SqlBaseParser(tokenStream);\n<line9>    parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line10>    parser.removeErrorListeners();\n<line11>    parser.addErrorListener(ERROR_LISTENER);\n<line12>    parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line13>    if (DEBUG) {\n<line14>        debug(parser);\n<line15>        tokenStream.fill();\n<line16>        for (Token t : tokenStream.getTokens()) {\n<line17>            String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line18>            String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line19>        }\n<line20>    }\n<line21>    ParserRuleContext tree = parseFunction.apply(parser);\n<line22>    if (DEBUG) {\n<line23>    }\n<line24>    return visitor.apply(new AstBuilder(paramTokens, zoneId), tree);\n<line25>}","output":"<line1>{\n<line2>    SqlBaseLexer lexer = new SqlBaseLexer(new CaseChangingCharStream(CharStreams.fromString(sql), true));\n<line3>    lexer.removeErrorListeners();\n<line4>    lexer.addErrorListener(ERROR_LISTENER);\n<line5>    Map<Token, SqlTypedParamValue> paramTokens = new HashMap<>();\n<line6>    TokenSource tokenSource = new ParametrizedTokenSource(lexer, paramTokens, params);\n<line7>    CommonTokenStream tokenStream = new CommonTokenStream(tokenSource);\n<line8>    SqlBaseParser parser = new SqlBaseParser(tokenStream);\n<line9>    parser.addParseListener(new PostProcessor(Arrays.asList(parser.getRuleNames())));\n<line10>    parser.removeErrorListeners();\n<line11>    parser.addErrorListener(ERROR_LISTENER);\n<line12>    parser.getInterpreter().setPredictionMode(PredictionMode.SLL);\n<line13>    if (DEBUG) {\n<line14>        debug(parser);\n<line15>        tokenStream.fill();\n<line16>        for (Token t : tokenStream.getTokens()) {\n<line17>            String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line18>            String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line19>            log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line20>        }\n<line21>    }\n<line22>    ParserRuleContext tree = parseFunction.apply(parser);\n<line23>    if (DEBUG) {\n<line24>        log.info(\"Parse tree {} \" + tree.toStringTree());\n<line25>    }\n<line26>    return visitor.apply(new AstBuilder(paramTokens, zoneId), tree);\n<line27>}"},{"input":"","instruction":"<line1>{\n<line2>    debug(parser);\n<line3>    tokenStream.fill();\n<line4>    for (Token t : tokenStream.getTokens()) {\n<line5>        String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line6>        String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line7>    }\n<line8>}","output":"<line1>{\n<line2>    debug(parser);\n<line3>    tokenStream.fill();\n<line4>    for (Token t : tokenStream.getTokens()) {\n<line5>        String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line6>        String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line7>        log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line8>    }\n<line9>}"},{"input":"","instruction":"<line1>{\n<line2>    String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line3>    String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line4>}","output":"<line1>{\n<line2>    String symbolicName = SqlBaseLexer.VOCABULARY.getSymbolicName(t.getType());\n<line3>    String literalName = SqlBaseLexer.VOCABULARY.getLiteralName(t.getType());\n<line4>    log.info(format(Locale.ROOT, \"  %-15s '%s'\", symbolicName == null ? literalName : symbolicName, t.getText()));\n<line5>}"},{"input":"","instruction":"<line1>{\n<line2>}","output":"<line1>{\n<line2>    log.info(\"Parse tree {} \" + tree.toStringTree());\n<line3>}"}]